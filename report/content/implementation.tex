\section{Implementation} % (fold)
\label{sec:implementation}

Using the requirements set forth in Section~\ref{sec:dvpns} we can compile a list of contemporary technologies that can meet them and provide \acp{dvpn}. The protocols considered had to meet \ref{lst:num} criteria:
\begin{inparaenum}[\itshape 1\upshape)]
	\item can provide Ethernet \acp{ppvpn} between multiple sites, and
	\item protocol stack must be supported in hardware at time of writing.
	\label{lst:num}
\end{inparaenum}

\subsection{Contemporary Technologies} % (fold)
\label{sub:contemporary_technologies}

%\subsubsection{\acs{atm}} % (fold)
%\label{ssub:atm}
%For the sake of completeness, we will briefly look at \acs{atm}. \ac{atm} is a legacy protocol that has been used by operators to carry traffic over the internet backbone since the 1990s. Where as Ethernet and \ac{ip} are developed as packet-routing connection-less protocols, \ac{atm} is a cell-switched connection-oriented protocol. This poses a number of problems when trying to transport \ac{ip} over \ac{atm}. First, the variable length of the packets don't map efficiently to the fixed size cells of \ac{atm}. Drops of a single cell would cause the entire frame to become unusable. Then there is the added overhead of encapsulating \ac{ip} over \ac{atm}, which causes inefficient use of the network resources when compared to running an all \ac{ip} network. Finally, the \ac{qos} features of \ac{atm} are left unused \cite{atm}. These problems are some of the reasons that operators have moved away from \ac{atm} based backbones, to all \ac{ip} ones.
%
%% subsubsection atm (end)

\subsubsection{\acs{spb}} % (fold)
\label{ssub:spb}

\ac{spb} is an evolution of the original \acs{ieee} 802.1Q \ac{vlan} standard. \ac{vlan} tags have been in use in the networking world for a long time and provide decent separation in campus networks. However, when \ac{vlan}-tagging was done at the customer network, the carrier couldn't separate the traffic from different customers anymore. This resulted in 802.1Qad or Q-in-Q which added an S-\ac{vlan} tag to separate the client \acp{vlan} from the \ac{sp} \acp{vlan} in the backbone. This was usable for the Metro Ethernet networks for a while but when \acp{sp} started providing this services to more and more customers, their backbone switches could not keep up with the clients \ac{mac} addresses.

To provide the required scalability with regard to \acp{mac} in the backbone, \ac{pbb} (802.1Qay or \ac{mac}-in-\ac{mac}) was introduced. It encapsulates the whole Ethernet frame on the edge of the carrier network and forwards the frame based on the Backbone-\ac{mac} of the egress \ac{pe}. It also separated client \acp{vpn} using a Service Instance Identifier (I-SID), which with 24 bits is able to supply the carrier with 16 million separate networks. The downside of \ac{pbb} remained one that is common to all Layer 2 forwarding protocols: the possibility of loops. Preventing them requires \ac{stp} which will disable links to get a loop-free network. Disadvantages of \ac{stp} include the relatively long convergence time and inefficient use of resources due to the disabled links. This has been solved by using \acs{isis} as a routing protocol to discover the network topology. After which each \ac{pe} creates a \acp{spt} originating from each edge device to every other \ac{pe}. This is called \ac{spb} or 802.1aq.

\ac{spb} benefits from the maturity of the Ethernet protocol by reusing protocols for \ac{oam} and \ac{pm}. This allows for fast error detection and extensive troubleshooting tools by using the \ac{ieee} 802.1ag and \ac{itut} Y.1731 standards respectively. The \ac{isis} implementation has also been adapted to rapidly detect errors however, no fast recovery function has been defined, besides complete \ac{isis} reconvergence. This would result in a traffic impact of several hundreds of milliseconds in large networks \cite{spb-nanog}. 

However, due to its Ethernet \acs{stp} forwarding-based nature it lacks \ac{te} features. The paths that the \ac{vpn} traffic takes are not explicitly configureable and provide limited scalability due to limited amounts of available paths (or trees in this case). As such, operators can not define constraints or explicit paths to take over the network to distribute traffic in an efficient manner. The lack of available paths also negatively affects its \ac{ecmp} functionalities. However, future, additional algorithms with multiple paths maybe introduced using extensible \ac{ect} algorithms \cite{rfc6329}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=10cm]{./includes/spb-stack.pdf}
	\caption{Provisioning a \ac{dvpn} using \ac{spb}.}
	\label{fig:spb-stack}
\end{figure}

Provided that \ac{isis} has been configured on all provider devices, Figure~\ref{fig:spb-stack} illustrates the interfaces needed to provision a \ac{dvpn} in \ac{spb}. It excels in its simplicity by providing `single-point provisioning'. This means that the \ac{nms} only needs to add the member \ac{ce} port to a certain \ac{dvpn} I-SID, after which the \ac{pe} floods this binding through the \ac{isis} network and other \acp{pe} sharing this I-SID will install the path towards the ingress \ac{pe}. The rate limiting of the \ac{ce} ports is a vendor-specific feature however, and may vary per hardware platform.


% subsubsection spb (end)

\subsubsection{\acs{mpls}} % (fold)
\label{ssub:mpls}

\ac{mpls} is known for its scalability and extensibility. Over the past decade additions have been made to the original specification to overcome a plethora of issues within carrier networks. This initially started with trying to implement fast forwarding in legacy switches using labels (or tags) at the start of the frame \cite{tag-switching}. When this issue became surmountable using new hardware, \ac{mpls} had already proven to be capable of transporting a wide arrange of protocols on the carrier backbone network, all the while also providing scalability, \ac{te} and \ac{qos} features to the operators.

\ac{mpls} itself is more a way of forwarding frames through the network, without facilitating any topology discovery, route determination, resource management, etc. These functions are left to a stack of other protocols. Without \ac{ip} reachability throughout the network these protocols cannot exchange traffic and so, as a prerequisite, \ac{mpls} relies on an \ac{igp} like \ac{ospf} to discovery the topology.

The distribution of labels has to be facilitated as well, which is done using \ac{ldp} and/or \ac{rsvp}. These protocols run between each device in the path between two \acp{pe} and exchange the labels that the will assign to a certain path, thereby setting up a \ac{lsp}. \ac{ldp} does this by distributing its labels from the egress \ac{pe} up towards the ingress \ac{pe} based on \ac{igp} costs. \ac{rsvp}, on the other hand, signals its paths from the ingress \ac{pe} towards the downstream \ac{pe} based on constraints, potential explicit hops or as a last resort using the \ac{igp} next hop. Label distribution is still determined from egress to ingress, but the actual path is determined at the head-end. To determine the best path to take, \ac{rsvp} uses the \ac{cspf} algorithm which can take into account link characteristics like bandwidth or \ac{frr} support. This allows \ac{rsvp} \acp{lsp} to take more well informed paths through the network and together with support for defining explicit paths, allows for granular \ac{te} features which \ac{ldp} lacks. Both \ac{ldp} and \ac{rsvp} also allow for the use of multiple paths over the network to share traffic load towards a \ac{pe}.

The aforementioned \ac{frr} feature is unique to \ac{rsvp} and provides the network with fast failure recovery. It does so by preprovisioning a so-called backup \ac{lsp} next to the primary \ac{lsp}. When a failure is detected on the primary \ac{lsp}, traffic is immediately shifted towards the standby path, yielding a sub-50ms failover. Obviously, this value also depends on the time it takes for the failure to be detected. Therefore it is important to have some sort of detection mechanism in place. One that is commonly used and integrates with \ac{ospf} is \ac{bfd}. This protocol sets up sessions between devices and triggers an alarm when the session does not behave as expected. At which point \ac{frr} kicks in.

\acp{vpn} are also provided by additional protocols. Layer 3 \acp{vpn} make use of \ac{bgp} to distribute client prefixes to the edges of the carrier network. The core is only concerned with the forwarding of labels and has now knowledge of these \acs{ip} prefixes. Layer 2 \acp{vpn} make use of \ac{vpls}, a service which encapsulates the entire Ethernet frame and pushes a label to it to map it to a certain separated network. Again, the core is only concerned with the labels and only the edges need to know the clients \acs{mac} addresses. When setting up a \ac{vpls} instance (a \acp{vpn}), \ac{ldp} sessions are setup between all \acp{pe} part of the same \ac{vpls} instance. Consecutively, the \acp{pe} will exchange their chosen labels for that instance between each other.

The \acp{cmac} in a \ac{vpls} instance are normally learned through the data plane. That is, when a frame comes in from a \ac{ce}, the \ac{pe} learns the \ac{sa} behind the corresponding port. If it doesn't know the \ac{da}, it will flood the frame to other \acp{pe} with member ports in that instance. These \acp{pe} in turn learn the \ac{sa} as well behind the ingress \ac{pe}. When a large number of \acp{cmac} are present within a \ac{vpls} instance this can cause a lot of broadcast traffic, specifically \ac{arp} traffic. To solve this, the \ac{evpn} standard has been proposed \cite{evpn}. This technique provides \ac{mac} learning in the control plane by exchanging learned \acp{cmac} between \acp{pe} using Multi-Protocol \ac{bgp}. Additionally it may also learn the \ac{ip} address associated with the \ac{cmac} and distribute that as well. Thereby being able to act as an \ac{arp} proxy, as earlier illustrated in Figure~\ref{fig:arp-proxy}.

The different protocols all depend on each other, as illustrated in Figure~\ref{fig:mpls-stack}. Each \ac{pe} device runs this stack, while \ac{p} devices run a subset which is shaded. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=7cm]{./includes/mpls-stack.pdf}
	\caption{Dependency stack of \ac{mpls}-related technologies.}
	\label{fig:mpls-stack}
\end{figure} 

To configure a \ac{dvpn} using \ac{mpls} first the participating \acp{pe} need to be configured with the new \ac{vpls} instance to which the member \ac{ce} ports will be added. Next, constraints are defined by the \ac{nms}, which can be in the form of an explicit route to make a static route or by defining loose constraints based on bandwidth limits which can be used the \ac{cspf} algorithm. Using these constraints, paths are installed at each \ac{pe} towards every other participating \ac{pe}. These paths are then added to the \ac{vpls} instance, allowing \ac{ldp} sessions to be setup between the \acp{pe}. Next, for \ac{frr}, backup \acp{lsp} need to defined similarly to the primary \ac{lsp} but over a different path, which can again be done using constraints to exclude the other links. Utilization of the links in the network has to be monitored as well and when a path has a link which is nearing capacity, new \acp{lsp} have to be provisioned and some \ac{vpls} paths move to those \acp{lsp}. And finally the ingress traffic on the \ac{ce} ports need to be rate limited. This procedure again, is not standardized and is dependent on support of the hardware.

The procedure above implies that the backbone network has been setup with the following protocols and features already enabled: \ac{ip} addressing, \ac{ospf} routing, \ac{mpls} forwarding, \ac{rsvp} with \ac{frr} and \ac{bfd}. After initial setup of the backbone network the \ac{nms} is only concerned with the \acp{pe}, as can also be seen in Figure~\ref{fig:nms-stack}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=10cm]{./includes/nms-stack.pdf}
	\caption{Provisioning a \ac{dvpn} using \ac{mpls}.}
	\label{fig:nms-stack}
\end{figure}

% subsubsection mpls (end)

% subsection contemporary_technologies (end)

\subsection{OpenFlow} % (fold)
\label{sub:openflow}

Section~\ref{sec:introduction} gave a short introduction into what \ac{sdn} and OpenFlow entail and what it promises in terms of cost savings and agility. \acl{sdn} is the general principle of designing flexible networks using open interfaces towards the hardware. OpenFlow is a subcomponent of this new principle which provides a protocol between the forwarding plane of the networking devices and a centralized controller. Essentially, taking over the role of the distributed control planes of the network devices. A general overview of the \ac{sdn} architecture and OpenFlow is given in Figure~\ref{fig:of-arch}. OpenFlow provides the controller with an \ac{api} that can be used to install flow entries directly in the forwarding plane of the devices. Flow entries consist of six fields:

\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Instructions}]
	\item[Match] This field contains a list of frame/packet characteristics that will need to be present to match to this entry, e.g.\ \ac{da}, \ac{ip} source address or \ac{mpls} tags.
	\item[Priority] The precedence of this flow entry over other flow entries to which a certain frame matches.
	\item[Counters] Frames matching to this entry are counted for monitoring purposes.
	\item[Instructions] When a frame is matched using the match field, it is processed according to a list of instructions, which may include forwarding out of a port, rewriting headers and/or applying meters.
	\item[Timeouts] The time that a flow entry can be live until it is discarded.
	\item[Cookie] Value assigned by controller to identify the flow (not used to forward frames).
\end{description}

\begin{figure}[!h]
	\centering
	\includegraphics[width=7cm]{./includes/of-arch.pdf}
	\caption{Architecture of \ac{sdn} and OpenFlow.}
	\label{fig:of-arch}
\end{figure}

Flow entries have been understandably compared to \acs{acl} entries, however it is also apparent that there are many more possibilities with regards to matching and performing actions. The frame fields that can be matched upon have changed over the lifetime of the OpenFlow specification. For example, version 1.0 could only match and/or act upon tagged traffic using a single outer-\acs{vlan} tag. Version 1.1 added matches and actions for Q-in-Q tags and \ac{mpls} labels, and version 1.3 could also match \ac{pbb} tags. See Table~\ref{tb:of-versions} for a comparison of key features in different versions of OpenFlow spec. 

\begin{table}[!h]
	\centering
	\begin{tabular}{r|cccc}
	 			& 1.0 & 1.1 & 1.2 & 1.3 \\
	\hline
	VLAN Tags 	& \checkmark & \checkmark & \checkmark & \checkmark \\
	Q-in-Q Tags &   & \checkmark & \checkmark & \checkmark \\
	MPLS Tags 	&   & \checkmark & \checkmark & \checkmark \\
	PBB Tags 	&   &   &   & \checkmark \\
	Groups 		&   & \checkmark & \checkmark & \checkmark \\
	Rate limiting & \checkmark & \checkmark & \checkmark & \checkmark  \\
	\end{tabular}
	\caption{Comparison of OpenFlow versions regarding key features for \acp{dvpn}.}
	\label{tb:of-versions}	
\end{table}

Note that rate limiting on a per port basis has been available in OpenFlow since version 1.0. Additionally, version 1.3 added support for per flow rate limiting using so called `meters' which can be assigned to specific flows. By doing so it becomes possible to also rate limit flows on certain aggregation ports rather than just at the ingress port of the \ac{ce}.

The installation of flow entries is done by the controller, governed by the applications running on it. Applications can be written to provide functions like topology discovery, routing, etc. Moreover, without these installed applications, the network will be unable to forward any traffic. The interface between the applications and the controller is also being referred to as the `northbound interface'. This interface, in contrast to the southbound OpenFlow interface, has not been specified and varies between different controller implementations, limiting the portability of the network applications.

Fast failover and \ac{ecmp} can be accomplished using the aforementioned port `groups', which are available since version 1.1. Groups can be defined as a destination in a flow entry and contains a list of ports. The type of group defines the action of the group: `\textbf{all}' sends the frame out the frame out of all ports in the group (broadcast/multicasting); `\textbf{select}' outputs it to one of the ports (providing \ac{ecmp}); `\textbf{indirect}' is a single port group which can be used by multiple flow entries (aggregation); and `\textbf{fast failover}' which choses the first \textsl{live} port out of which it will forward the frame. To support `fast failover' a \textsl{liveness monitoring} technique needs to be implemented supported by the switch. This could simply be the physical link state or more intricate tools, e.g.\ \ac{bfd}.

Implementing \acp{dvpn} using OpenFlow relies mostly on the applications running on the controller. When they are written and configured as desired, provisioning a \ac{dvpn} would only require the input of the data. After which the applications and controller install flow entries into all the network devices in the \ac{dvpn} path, as can be seen in Figure~\ref{fig:nms-stack-of}.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{./includes/nms-stack-of.pdf}
	\caption{Provisioning a \ac{dvpn} using OpenFlow.}
	\label{fig:nms-stack-of}
\end{figure}

\subsubsection{\acs{dvpn} Applications} % (fold)
\label{ssub:dvpn_application}


To implement \acp{dvpn}, a combination of applications need to be installed on the controller that contemporary technologies solve using distributed protocols:

\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries Topology Discovery}]
	\item[Topology Discovery] Network devices do not require \ac{ip} connectivity between each other to exchange route information. Instead, they rely on their connection to the controller to provide them with information. A topology discovery applications will instruct network devices to send out unique discovery messages which are then received by other devices, which forward the message back up to the controller. By keeping information on which packet goes in and comes out where, the application can get an overview of the network.
	\item[\ac{dvpn} Provisioning] The \ac{dvpn} input will provide the application with at least the \ac{ce} port and preferably the corresponding \ac{mac} and/or \ac{ip} address. It can then instruct the path provisioning component to setup a path between the \acp{pe} participating in the \ac{dvpn}. The application is also concerned with keep the provided or dynamically learned \ac{mac} addresses up-to-date. Additionally, a flow can be installed matching on the \ac{arp} EtherType that sends the \ac{arp} request towards the controller. If the application also keeps track of the \ac{ip} addresses of the \acp{ce} it can then act as an \ac{arp} proxy and reply to the requesting \ac{ce} with the correct \ac{ip} address.
	\item[Path Provisioning] Using the discovered topology, paths are then setup over those routes between \acp{pe} with member \ac{ce} ports in a common \ac{dvpn}. To provide \ac{mac} scalability over the backbone network, the traffic will need to be tagged using either \ac{pbb} or \ac{mpls} labels. In contrast with the contemporary technologies the \ac{p} devices also need to be updated using the forwarding information. After all, there are no distribution protocols running between the devices.
	\item[Traffic Engineering] The path setup procedure uses data from the \ac{dvpn} input and the discovered network resources to provide the most optimal path between two \acp{pe}. Constraints for the paths taken by each \ac{dvpn} can be configured by the operator and influence the route selection directly over the whole platform. Also, using input from the \ac{oam} monitoring applications paths may be preferred or deprecated based on their performance.
	\item[\acs{oam}] The controller and applications provide a complete overview of the network but troubleshooting and monitoring still has to be done at the network level as well. Different approaches can be taken to do so, one of which could be sending out periodic keep-alive messages in the same path from the controller down to the ingress \ac{pe} that the egress \ac{pe} should forward back up to the controller. Another example is implementing an already defined \ac{oam} protocol in OpenFlow, as has been done in \cite{of-oam} which implemented Ethernet 802.1ag \ac{oam}.
\end{description}

The interaction between the different applications has been illustrated in Figure~\ref{fig:dvpn-apps}.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{./includes/dvpn-apps.pdf}
	\caption{Interactions between applications to implement \acp{dvpn}.}
	\label{fig:dvpn-apps}
\end{figure}

Unlike contemporary technologies that require inter-device communication before any paths can be set up, the forwarding tables of OpenFlow devices are empty. The only prerequisite is that the devices all have a management connection to the controller from which they can receive their forwarding information.

% subsubsection dvpn_applications (end)


% subsection openflow (end)

% section implementation (end)
