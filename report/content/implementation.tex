\section{Implementation} % (fold)
\label{sec:implementation}

Using the requirements set forth in Section~\ref{sec:dvpns} we can compile a list of contemporary technologies that can meet them and provide \acp{dvpn}. The protocols considered had to meet \ref{lst:num} criteria:
\begin{inparaenum}[\itshape 1\upshape)]
	\item can provide Ethernet \acp{ppvpn} between multiple sites, and
	\item protocol stack must be supported in hardware at time of writing.
	\label{lst:num}
\end{inparaenum}





what can provide what function for DVPNs?

\subsection{Contemporary Technologies} % (fold)
\label{sub:contemporary_technologies}

%\subsubsection{\acs{atm}} % (fold)
%\label{ssub:atm}
%For the sake of completeness, we will briefly look at \acs{atm}. \ac{atm} is a legacy protocol that has been used by operators to carry traffic over the internet backbone since the 1990s. Where as Ethernet and \ac{ip} are developed as packet-routing connection-less protocols, \ac{atm} is a cell-switched connection-oriented protocol. This poses a number of problems when trying to transport \ac{ip} over \ac{atm}. First, the variable length of the packets don't map efficiently to the fixed size cells of \ac{atm}. Drops of a single cell would cause the entire frame to become unusable. Then there is the added overhead of encapsulating \ac{ip} over \ac{atm}, which causes inefficient use of the network resources when compared to running an all \ac{ip} network. Finally, the \ac{qos} features of \ac{atm} are left unused \cite{atm}. These problems are some of the reasons that operators have moved away from \ac{atm} based backbones, to all \ac{ip} ones.
%
%% subsubsection atm (end)

\subsubsection{\acs{spb}} % (fold)
\label{ssub:spb}

\ac{spb} is an evolution of the original \acs{ieee} 802.1Q \ac{vlan} standard. \ac{vlan} tags have been in use in the networking world for a long time and provide decent separation in campus networks. However, when \ac{vlan}-tagging was done at the customer network, the carrier couldn't separate the traffic from different customers anymore. This resulted in 802.1Qad or Q-in-Q which added an S-\ac{vlan} tag to separate the client \acp{vlan} from the \ac{sp} \acp{vlan} in the backbone. This was usable for the Metro Ethernet networks for a while but when \acp{sp} started providing this services to more and more customers, their backbone switches could not keep up with the clients \ac{mac} addresses.

To provide the required scalability with regard to \acp{mac} in the backbone, \ac{pbb} (802.1Qay or \ac{mac}-in-\ac{mac}) was introduced. It encapsulates the whole Ethernet frame on the edge of the carrier network and forwards the frame based on the Backbone-\ac{mac} of the egress \ac{pe}. It also separated client \acp{vpn} using a Service Instance Identifier (I-SID), which with 24 bits is able to supply the carrier with 16 million separate networks. The downside of \ac{pbb} remained one that is common to all Layer 2 forwarding protocols: the possibility of loops. Preventing them requires \ac{stp} which will disable links to get a loop-free network. Disadvantages of \ac{stp} include the relatively long convergence time and inefficient use of resources due to the disabled links. This has been solved by using \acs{isis} as a routing protocol to discover the network topology. After which each \ac{pe} creates a \acp{spt} originating from each edge device to every other \ac{pe}. This is called \ac{spb} or 802.1aq.

\ac{spb} benefits from the maturity of the Ethernet protocol by reusing protocols for \ac{oam} and \ac{pm}. This allows for fast error detection and extensive troubleshooting tools by using the \ac{ieee} 802.1ag and \ac{itut} Y.1731 standards respectively. The \ac{isis} implementation has also been adapted to rapidly detect errors however, no fast recovery function has been defined, besides complete \ac{isis} reconvergence. This would result in a traffic impact of several hundreds of milliseconds in large networks \cite{spb-nanog}. 

However, due to its Ethernet \acs{stp} forwarding-based nature it lacks \ac{te} features. The paths that the \ac{vpn} traffic takes are not explicitly configureable and provide limited scalability due to limited amounts of available paths (or trees in this case). As such, operators can not define constraints or explicit paths to take over the network to distribute traffic in an efficient manner. The lack of available paths also negatively affects its \ac{ecmp} functionalities. However, future, additional algorithms with multiple paths maybe introduced using extensible \ac{ect} algorithms \cite{rfc6329}.

Provided that \ac{isis} has been configured on all provider devices, Figure~\ref{fig:spb-stack} illustrates the interfaces needed to provision a \ac{dvpn} in \ac{spb}. First, the I-SID has to be defined on all \acp{pe}, which in turn will setup \acp{spt} towards the other participating \acp{pe}. The member \ac{ce} ports then need to be added to the I-SID after which the \ac{vpn} is established. The rate limiting of the \ac{ce} ports is a vendor-specific feature however, and may vary per hardware platform.

\begin{figure}[!h]
	\centering
	\includegraphics[width=9cm]{./includes/spb-stack.pdf}
	\caption{Provisioning a \ac{dvpn} using \ac{spb}.}
	\label{fig:spb-stack}
\end{figure}


% subsubsection spb (end)

\subsubsection{\acs{mpls}} % (fold)
\label{ssub:mpls}

\ac{mpls} is known for its scalability and extensibility. Over the past decade additions have been made to the original specification to overcome a plethora of issues within carrier networks. This initially started with trying to implement fast forwarding in legacy switches using labels (or tags) at the start of the frame \cite{tag-switching}. When this issue became surmountable using new hardware, \ac{mpls} had already proven to be capable of transporting a wide arrange of protocols on the carrier backbone network, all the while also providing scalability, \ac{te} and \ac{qos} features to the operators.

\ac{mpls} itself is more a way of forwarding frames through the network, without facilitating any topology discovery, route determination, resource management, etc. These functions are left to a stack of other protocols. Without \ac{ip} reachability throughout the network these protocols cannot exchange traffic and so, as a prerequisite, \ac{mpls} relies on an \ac{igp} like \ac{ospf} to discovery the topology.

The distribution of labels has to be facilitated as well, which is done using \ac{ldp} and/or \ac{rsvp}. These protocols run between each device in the path between two \acp{pe} and exchange the labels that the will assign to a certain path, thereby setting up a \ac{lsp}. \ac{ldp} does this by distributing its labels from the egress \ac{pe} up towards the ingress \ac{pe} based on \ac{igp} costs. \ac{rsvp}, on the other hand, signals its paths from the ingress \ac{pe} towards the downstream \ac{pe} based on constraints, potential explicit hops or as a last resort using the \ac{igp} next hop. Label distribution is still determined from egress to ingress, but the actual path is determined at the head-end. To determine the best path to take, \ac{rsvp} uses the \ac{cspf} algorithm which can take into account link characteristics like bandwidth or \ac{frr} support. This allows \ac{rsvp} \acp{lsp} to take more well informed paths through the network and together with support for defining explicit paths, allows for granular \ac{te} features which \ac{ldp} lacks. Both \ac{ldp} and \ac{rsvp} also allow for the use of multiple paths over the network to share traffic load towards a \ac{pe}.

The aforementioned \ac{frr} feature is unique to \ac{rsvp} and provides the network with fast failure recovery. It does so by preprovisioning a so-called backup \ac{lsp} next to the primary \ac{lsp}. When a failure is detected on the primary \ac{lsp}, traffic is immediately shifted towards the standby path, yielding a sub-50ms failover. Obviously, this value also depends on the time it takes for the failure to be detected. Therefore it is important to have some sort of detection mechanism in place. One that is commonly used and integrates with \ac{ospf} is \ac{bfd}. This protocol sets up sessions between devices and triggers an alarm when the session does not behave as expected. At which point \ac{frr} kicks in.

\acp{vpn} are also provided by additional protocols. Layer 3 \acp{vpn} make use of \ac{bgp} to distribute client prefixes to the edges of the carrier network. The core is only concerned with the forwarding of labels and has now knowledge of these \acs{ip} prefixes. Layer 2 \acp{vpn} make use of \ac{vpls}, a service which encapsulates the entire Ethernet frame and pushes a label to it to map it to a certain separated network. Again, the core is only concerned with the labels and only the edges need to know the clients \acs{mac} addresses. When setting up a \ac{vpls} instance (a \acp{vpn}), \ac{ldp} sessions are setup between all \acp{pe} part of the same \ac{vpls} instance. Consecutively, the \acp{pe} will exchange their chosen labels for that instance between each other.

The \acp{cmac} in a \ac{vpls} instance are normally learned through the data plane. That is, when a frame comes in from a \ac{ce}, the \ac{pe} learns the \ac{sa} behind the corresponding port. If it doesn't know the \ac{da}, it will flood the frame to other \acp{pe} with member ports in that instance. These \acp{pe} in turn learn the \ac{sa} as well behind the ingress \ac{pe}. When a large number of \acp{cmac} are present within a \ac{vpls} instance this can cause a lot of broadcast traffic, specifically \ac{arp} traffic. To solve this, the \ac{evpn} standard has been proposed \cite{evpn}. This technique provides \ac{mac} learning in the control plane by exchanging learned \acp{cmac} between \acp{pe} using Multi-Protocol \ac{bgp}. Additionally it may also learn the \ac{ip} address associated with the \ac{cmac} and distribute that as well. Thereby being able to act as an \ac{arp} proxy, as earlier illustrated in Figure~\ref{fig:arp-proxy}.

The different protocols all depend on each other, as illustrated in Figure~\ref{fig:mpls-stack}. Each \ac{pe} device runs this stack, while \ac{p} devices run a subset which is shaded. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=7cm]{./includes/mpls-stack.pdf}
	\caption{Dependency stack of \ac{mpls}-related technologies.}
	\label{fig:mpls-stack}
\end{figure} 

To configure a \ac{dvpn} using \ac{mpls} first the participating \acp{pe} need to be configured with the new \ac{vpls} instance to which the member \ac{ce} ports will be added. Next, constraints are defined by the \ac{nms}, which can be in the form of an explicit route to make a static route or by defining loose constraints based on bandwidth limits which can be used the \ac{cspf} algorithm. Using these constraints, paths are installed at each \ac{pe} towards every other participating \ac{pe}. These paths are then added to the \ac{vpls} instance, allowing \ac{ldp} sessions to be setup between the \acp{pe}. Next, for \ac{frr}, backup \acp{lsp} need to defined similarly to the primary \ac{lsp} but over a different path, which can again be done using constraints to exclude the other links. Utilization of the links in the network has to be monitored as well and when a path has a link which is nearing capacity, new \acp{lsp} have to be provisioned and some \ac{vpls} paths move to those \acp{lsp}. And finally the ingress traffic on the \ac{ce} ports need to be rate limited. This procedure again, is not standardized and is dependent on support of the hardware.

The procedure above implies that the backbone network has been setup with the following protocols and features already enabled: \ac{ip} addressing, \ac{ospf} routing, \ac{mpls} forwarding, \ac{rsvp} with \ac{frr} and \ac{bfd}. After initial setup of the backbone network the \ac{nms} is only concerned with the \acp{pe}, as can also be seen in Figure~\ref{fig:nms-stack}.

\begin{figure}[!h]
	\centering
	\includegraphics[width=9cm]{./includes/nms-stack.pdf}
	\caption{Provisioning a \ac{dvpn} using \ac{mpls}.}
	\label{fig:nms-stack}
\end{figure}

% subsubsection mpls (end)

% subsection contemporary_technologies (end)

\subsection{OpenFlow} % (fold)
\label{sub:openflow}

Section~\ref{sec:introduction} gave a short introduction into what \ac{sdn} and OpenFlow entail and what it promises in terms of cost savings and agility. \acl{sdn} is the general principle of designing flexible networks using open interfaces towards the hardware. OpenFlow is a subcomponent of this new principle which provides a protocol between the forwarding plane of the networking devices and a centralized controller. Essentially, taking over the role of the distributed control planes of the network devices. OpenFlow provides the controller with an \ac{api} that can be used to install flow entries directly in the forwarding plane of the devices (also called the `southbound interface'). Flow entries consist of three fields:

\begin{description}
	\item[Match] This field contains a list of frame/packet characteristics that will need to be present to match to this entry, e.g.\ \ac{da}, \ac{ip} source address or \ac{mpls} tags.
	\item[Actions] When a frame is matched using the match field, it is processed according to an action list. Actions may include forwarding out of a port, rewriting headers and/or popping labels.
	\item[Counters] Frames matching to this entry are counted for monitoring purposes.
\end{description}

Flow entries have been understandably compared to \acs{acl} entries, however it is also apparent that there are many more possibilities with regards to matching and performing actions. The frame fields that can be matched upon has changed over the lifetime of the OpenFlow specification. For example, version 1.0 could only match or act upon tagged traffic using a single outer-\acs{vlan} tag. Version 1.1 added matches actions for Q-in-Q tags and \ac{mpls} labels, and version 1.3 could also match \ac{pbb} tags. 

Also not included in version 1.0 were mechanisms to allow for fast failover and \ac{ecmp}. These were added in version 1.1 by introducing logical groups of ports to which a flow entry can forward the frame. However, to support fast failover a \textsl{liveness monitoring} technique will need to be implemented supported by the switch. This could simply be the physical link state or more intricate tools, e.g.\ \ac{bfd}. Metering and rate limiting of traffic per flow was also not specified until version 1.3. See Table~\ref{tb:of-versions} for a comparison of key features in different versions of OpenFlow spec. 

\begin{table}[h]
	\centering
	\begin{tabular}{r|cccc}
	 			& 1.0 & 1.1 & 1.2 & 1.3\\
	\hline
	VLAN Tags 	& x & x & x & x \\
	Q-in-Q Tags &   & x & x & x \\
	MPLS Tags 	&   & x & x & x \\
	PBB Tags 	&   &   &   & x \\
	Groups 		&   & x & x & x \\
	Metering 	&   &   &   & x \\
	\end{tabular}
	\caption{Comparison of OpenFlow version regarding key features for \acp{dvpn}.}
	\label{tb:of-versions}	
\end{table}

Installation of flow entries is done by the controller, governed by the applications running on it. Applications can be written to provide functions like topology discovery, routing, etc. Moreover, without these installed applications, the network will be unable to forward any traffic. The interface between the applications and the controller is also being referred to as the `northbound interface'. This interface, in contrast to the southbound OpenFlow interface, has not been specified and varies between different controller implementations, limiting the portability of the network applications.

To implement \acp{dvpn}, a couple of applications need to be written that contemporary technologies implement using available protocols:

\begin{description}
	\item[Topology Discovery] Network devices do not require \ac{ip} connectivity between each other to exchange route information. Instead, they rely on their connection to the controller to provide them with information. A topology discovery applications will instruct network devices to send out unique discovery messages which are then received by other devices, which forward the message back up to the controller. By keeping information on which packet goes in and comes out where, the application can get an overview of the network.
	\item[Path Provisioning] When the topology is known, another application can use this information to set up routes between \acp{pe} with member \ac{ce} ports in a common \ac{dvpn}. To provide \ac{mac} scalability over the backbone network, the traffic will need to be tagged not using C-\ac{vlan} tags but tags like \ac{pbb} or \ac{mpls} labels. In contrast with the contemporary technologies the \ac{p} routers also need to be updated using the forwarding information. After all, there are no distribution protocols running between the devices.
	\item[Traffic Engineering] The path setup procedure can use data from the \ac{dvpn} and the discovered network resources to provide the most optimal path between the two \acp{pe}. Constraints for the paths taken by each \ac{dvpn} can be configured by the operator and influence the route selection directly over the whole platform.
	\item[\acs{oam}] The controller and applications provide a complete overview of the network but troubleshooting and monitoring still has to be done at the network level as well. Different approaches can be taken to do so, one of which could be sending out periodic keep-alive messages in the same path from the controller down to the ingress \ac{pe} that the egress \ac{pe} should forward back up to the controller. Another example is implementing an already defined \ac{oam} protocol in OpenFlow, as has been done in \cite{of-oam} which implemented Ethernet 802.1ag \ac{oam}.
	\item[\acs{cmac} Distribution] 
\end{description}

Fast failover of routes and \ac{ecmp} can be done using the aforementioned port `groups', which are available since version 1.1. 

This means that through protocols such as \ac{rsvp}-TE, operators are trying to get control over the lower levels of the networking stack. 

The OpenFlow controller provides operators with an alternative to these interfaces, namely a programmable forwarding plane.

\begin{figure}[!h]
	\centering
	\includegraphics[width=9cm]{./includes/nms-stack-of.pdf}
	\caption{Provisioning a \ac{dvpn} using OpenFlow.}
	\label{fig:nms-stack-of}
\end{figure}

% subsection openflow (end)

\HRule

The features and limitations of the three discussed technologies are given in Table~\ref{tb:reqs}. They are compared to the list of requirements as discussed in Section~\ref{sec:dvpns}. Section~\ref{sec:results} will continue with an evaluation of the three architectures.

\begin{table}[h]
	\centering
	\begin{tabular}{r|lll}
	 & \acs{spb} & \acs{mpls} & OpenFlow / \acs{sdn}\\
	\hline
	Tagging of VPN Traffic & \acs{pbb} & \acs{vpls} & \acs{pbb} / \acs{mpls}\\
	MAC Scalability & yes & yes & yes\\
	Topology Discovery & \acs{isis} & \acs{ospf} & application\\
	Path Provisioning & \acs{spt} & \acs{rsvp} / \acs{ldp} & application\\
	Traffic Engineering & no & \acs{rsvp} & application\\
	\ac{ecmp} & limited & yes & yes, using Groups\\
	\ac{bum} limiting & dependent on \acs{hw} & dependent on \acs{hw} & yes, using Metering\\
	Exchange \acsp{cmac} & no & \ac{evpn} (draft) & application\\
	Traffic Rate Limiting & dependent on \acs{hw} & dependent on \acs{hw} & yes, using Metering\\
	Fast Failover & no & \acs{frr} & yes, using Groups\\
	\acs{oam} & 802.1ag & \acs{lsp} Ping / \acs{bfd} & application\\
	\hline
	Forwarding Decision & \acs{pbb} tags & \acs{mpls} labels & flow entry \\
	\ac{bum} traffic handling & flood & flood & sent to controller\\
	\end{tabular}
	\caption{Feature requirements available in discussed technologies.}
	\label{tb:reqs}
\end{table}

% section implementation (end)
